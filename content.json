{"meta":{"title":"milkcoffee","subtitle":"Miss he's Blog","description":"hetingting的个人博客","author":"和婷婷","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2020-08-08T13:19:19.000Z","updated":"2020-08-08T13:19:19.969Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"传统的机器学习方法——决策树（下）","date":"2020-08-10T09:40:25.601Z","updated":"2020-08-10T09:42:21.579Z","comments":true,"path":"2020/08/10/传统的机器学习方法——决策树（下）/","link":"","permalink":"http://yoursite.com/2020/08/10/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"title： 传统的机器学习方法——决策树（下）1 决策树的剪枝1.1 剪枝介绍在决策树学习中将已生成的树进行简化的过程称为剪枝。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型,这就是决策树的剪枝。 1.2 损失函数1.2.1 定义决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树T的叶结点个数为 |T|，t 是树 T 的叶结点，该叶结点有 Nt 个样本点，其中 k 类的样本点有 Ntk 个，k=1,2,…,K, Ht(T) 为叶结点 t 上的经验熵，α&gt;=0为参数，则决策树学习的损失函数可以定义为其中经验熵为：在损失函数中，将式子右端的第1项记作这时有 上式中，C(T) 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T| 表示模型复杂度，参数 α&gt;=0 控制两者之间的影响。较大的促使选择较简单的模型(树)，较小的 α 促使选择较复杂的模型(树)。α=0 意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 1.2.2 损失函数的优点剪枝，就是当 α 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 α 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好，损失函数正好表示了对两者的平衡。 1.3 剪枝算法输入：生成算法产生的整个树 T，参数 α；输出：修建后的子树 Tα。 计算每个结点的经验熵； 递归地从树的叶结点向上回缩。设一组叶结点回缩到父结点之前与之后的整体树分别为 Tb 与 Ta ，其对应的损失函数值分别时 Cα(Tb) 与 Cα(Ta) ，如果 Cα(Ta)&lt;=Cα(Tb) ,则进行剪枝，即将父结点变成新的叶结点。 返回2，直至不能继续为止，得到损失函数最小的子树 Tα。2 CART算法2.1 算法介绍CART算法由一下两步组成： 决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大； 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。 2.2 CART生成2.2.1 介绍决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。 2.2.2 回归树的生成假设 X 与 Y 分别为输入和输出变量，并且 Y 是连续变量，给定训练数据集考虑如何生成回归树。一个回归树对应着输入空间的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为 M 个单元 R1，R2 ，…..,Rm，并且在每个单元 Rm 上有一个固定的输出值 Cm,于是回归树模型可表示为当输入空间的划分确定时，可以用平方误差来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。平方误差如下: 2.2.2.1 最小二乘回归树生成算法输入：训练数据集 D；输出：回归树 f(x)。在训练数据集所在的输入空间中，递归地将每个区域划分为两个区域并决定每个子区域上的输出值，构建二叉决策树： 选择最优切分变量 j 与切分点 s ，求解：遍历变量 j ，对固定的切分变量 j 扫描切分点 s ，选择使上式达到最小值的对 (j,s)。 用选定的对 (j,s) 划分区域并决定相应的输出值： 继续对两个子区域调用步骤 1 ，2 ，直至满足停止条件。 将输入空间划分为 M 个区域 R1,R2,…,Rm,生成决策树： 2.2.3 分类树的生成分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。 2.2.3.1 基尼指数分类问题中，假设有 K 个类，样本点属于第 k 类的概率为 Pk,则概率分布的基尼指数定义为对于二类分类问题，若样本点属于第1个类的概率是 p，则概率分布的基尼指数为对于给定的样本集合 D，其基尼指数为这里，Ck是 D 中属于第 k 类的样本子集，K 是类的个数。如果样本集合 D 根据特征 A 是否取某一可能 α 被分割成 D1和 D2 两部分，即则在特征 A 的条件下，集合 D 的基尼指数定于为基尼指数表示集合 D 的不确定性，基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。 2.3 CART生成算法输入：训练数据集 D ，停止计算的条件；输出： CART 决策树。根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树： 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每一个特征，对其可能取得每个值a，根据样本点对A=a的测试为“是”或“否”将D分割成两部分，利用基尼指数公式计算A=a时的基尼指数。 对所有可能的特征A以及他们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最有特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 对两个子结点递归地调用1,2，直到满足停止条件。 生成CART决策树。 3 CART剪枝CART剪枝算法：输入：CART算法生成的决策树 T0；输出：最优决策树 Tα。（1）设k=0, T=T0（2）设α=+∞（3）自下而上地对各个内部结点t计算C(Tt)，|Tt| 以及这里，Tt 表示以 t 为根结点的子树，C(Tt) 是对训练数据的预测误差，|Tt| 是 Tt 的叶结点个数。（4）自上而下地访问内部结点 t ，如果有 g(t) = α ,进行剪枝，并对叶结点 t 以多数表决法表决其类，得到树 T 。（5）设 k=k+1, αk=α, Tk=T。（6）如果 T 不是由根结点单独构成的树，则回到步骤（4）.（7）采用交叉验证法在子树序列 T0,T1,…,Tn 中选取最优子树Tα。 参考文献：李航. 统计学习方法[M]. 北京：清华大学出版社，2012","categories":[],"tags":[]},{"title":"传统的机器学习方法——决策树（上）","slug":"传统的机器学习方法——决策树（上）","date":"2020-08-09T01:47:01.733Z","updated":"2020-08-09T01:52:18.752Z","comments":true,"path":"2020/08/09/传统的机器学习方法——决策树（上）/","link":"","permalink":"http://yoursite.com/2020/08/09/%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"@[toc] 1 决策树模型介绍1.1 决策树模型概述 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。 分类的时候，从根节点开始，当前节点设为根节点，当前节点必定是一种特征，根据实例的该特征的取值，向下移动，直到到达叶节点，将实例分到叶节点对应的类中。 1.2 决策树与if-then规则可以将决策树看成一个if-then规则的集合：由决策树的根节点到叶节点的每一条路径构建一条规则；路径上的内部结点的特征对应着if条件，叶节点对应着then结论。决策树的每一条路径都具有一个重要的性质：互斥且完备。这就是说，任何一个实例都被且仅被一条路径或规则覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。举个例子：if(明天是晴天）then(我将出去玩) 1.3 决策树主要优点1.分类速度快；2.具有可读性；3.学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新数据集，利用决策树模型进行分类。 1.4 主要步骤1.特征选择问题2.决策树的生成3.决策树的剪枝 2 特征选择2.1 特征选择问题特征选择在于选取对训练集具有分类能力的特征。如果利用一个特征进行分类的结果与随即分类的结果没有很大差别，则称这个特征没有分类能力。通常特征选则的准则是信息增益或信息增益比。特征选择是决定用哪个特征来划分特征空间。 2.1.1 举个例子下表（表5.1）是由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性），具体信息如表所示：通过上表所给的训练数据构建一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请是，根据申请人的特征利用决策树决定是否批准贷款申请。在说明这个例子之前，必须先了解几个定义。 2.2 重要定义2.2.1 熵由熵的定义可知，熵只依赖与X的分布，而与X的取值无关，所以也可以将X的熵记作H（p）,即 2.2.2 条件熵 2.2.3 信息增益一般地，熵H(Y)与条件熵H(Y|X)之差成为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 2.3 信息增益的算法2.3.1 算法过程 2.3.2 举个例子 3 决策树的生成3.1 介绍 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止，决策树停止生长。这个过程实际上就是使用满足划分准则的特征不断的将数据集划分成纯度更高，不确定性更小的子集的过程。对于当前数据集的每一次划分，都希望根据某个特征划分之后的各个子集的纯度更高，不确定性更小。 决策树生成算法不唯一，这里仅介绍两种算法——ID3算法和C4.5算法。 3.2 ID3算法3.2.1 ID3算法核心核心：是在决策树各个节点上应用**信息增益准则**选择特征递归地构建决策树。 3.2.2 算法过程算法的过程为： 1）初始化信息增益的阈值ϵ 2）判断样本是否为同一类Di ，如果是则返回单节点树T。标记类别为Di。 3）判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。 4）计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征Ag。 5）如果Ag的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。 6）否则，按特征Ag的不同取值Agi将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为Agi。返回增加了节点的数T。 7）对于所有的子节点，令D=Di,A=A−{Ag} ，递归调用2-6步，得到子树Ti并返回。 3.2.3 举个例子用ID3算法构建表5.1的决策树过程如下：构建决策树结果： 3.2.4 ID3算法的不足 ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。 ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑。 没有考虑过拟合的问题。 3.3 C4.5算法C4.5算法与ID3算法很相似，C4.5算法仅仅是对ID3算法做了改进，在生成决策树过程中采用信息增益比来选择特征算法过程没有变化，这里不再赘述。 3.3.1 C4.5算法的不足 C4.5虽然改进了ID3算法的几个主要的问题，仍然有优化的空间。 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。 这4个问题在CART树里面部分加以了改进。所以目前如果不考虑集成学习话，在普通的决策树算法里，CART算法算是比较优的算法了。4 总结这篇文章仅简单介绍了决策树的基本内容，主要是决策树的特征选择和生成两部分，后续会继续介绍决策树的剪枝和CART这个比较方便的算法。","categories":[],"tags":[]},{"title":"python中的bug","slug":"python中的bug","date":"2020-08-08T12:26:01.980Z","updated":"2020-08-09T01:23:37.267Z","comments":true,"path":"2020/08/08/python中的bug/","link":"","permalink":"http://yoursite.com/2020/08/08/python%E4%B8%AD%E7%9A%84bug/","excerpt":"","text":"python中解决SyntaxError: Non-UTF-8 code starting with ‘\\xc7’ in file 的问题在pycharm中运行.py程序，出现如下问题：只需要一步就可以解决：在程序的第一行前面加上：**# coding=gbk** 注意：coding与=，=与gbk之间没有空格程序运行结果就可以出来（如下）：","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-08-02T00:23:27.482Z","updated":"2020-08-02T00:23:27.482Z","comments":true,"path":"2020/08/02/hello-world/","link":"","permalink":"http://yoursite.com/2020/08/02/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}